{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成方法\n",
    "集成方法的目标是将使用给定学习算法构建的几个基本估计量的预测结合起来，以提高单个估计量的通用性/鲁棒性。通常区分两种集成方法：(1)在平均方法中，驱动原理是独立建立多个估计量，然后平均其预测。平均而言，由于组合估计量的方差减小，因此组合估计量通常比任何单个基本估计量都要好。例如： 套袋方法，随机树木森林 …;(2)相比之下，在增强方法中，基础估计器是按顺序构建的，并且人们尝试减小组合估计器的偏差。这样做的动机是结合几个弱模型以产生强大的整体。例如： AdaBoost，梯度树增强 …\n",
    "\n",
    "\n",
    "### BaggingClassifier\n",
    "Bagging分类器是一个集合元估计器，它使每个基本分类器适合原始数据集的随机子集，然后将其单个预测（通过投票或平均）进行汇总以形成最终预测。通过将随机化引入其构造过程中，然后使其整体，这种元估计器通常可以用作减少黑匣子估计器（例如决策树）方差的方法。\n",
    "\n",
    "**参数：**\n",
    " - base_estimator:对象，默认=无;基础模型（或称为estimator），默认为None，使用决策树。\n",
    " - n_estimators:int，默认= 10;基础模型数量。\n",
    " - max_samples:int或float，默认为1.0;用来给每个子模型训练数据集抽取比例。如果是int，则表示数量，如果是float则表示比例。\n",
    " - max_features:int或float，默认= 1.0;特征子采样参数，如果是int，则表示数量，如果是float则表示比例。\n",
    " - bootstrap:bool，默认为True;采样时是否替换替换，默认执行不替换的采样。\n",
    " - bootstrap_features:bool，默认= False;是否替换特征。\n",
    " - oob_score: bool，默认= False;是否使用现成的样本来估计泛化误差。\n",
    " - warm_start: bool，默认为False;设置为True时，请重用上一个调用的解决方案以适合并为集合添加更多估计量，否则，仅适合一个全新的集合。\n",
    " - n_jobs: int，默认=无;使用线程数。-1表示所有，可能会对可视化编程的监控线程造成影响。\n",
    " - random_state: int或RandomState，默认=无;随机数种子。\n",
    " - verbose: int, 默认为;训练和测试过程展示的详细程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = BaggingClassifier(base_estimator=SVC(),\n",
    "                        n_estimators=10, random_state=0).fit(X, y)\n",
    "clf.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaggingRegressor\n",
    "Bagging回归器是一个集合元估计器，它使每个基本回归器适合原始数据集的随机子集，然后将其单个预测（通过投票或平均）进行汇总以形成最终预测。通过将随机化引入其构造过程中，然后使其整体，这种元估计器通常可以用作减少黑盒估计器（例如决策树）方差的方法。\n",
    "\n",
    "**参数：**\n",
    " - base_estimator:对象，默认=无;基础模型（或称为estimator），默认为None，使用决策树。\n",
    " - n_estimators:int，默认= 10;基础模型数量\n",
    " - max_samples:int或float，默认为1.0;用来给每个子模型训练数据集抽取比例。如果是int，则表示数量，如果是float则表示比例\n",
    " - max_features:int或float，默认= 1.0;特征子采样参数，如果是int，则表示数量，如果是float则表示比例\n",
    " - bootstrap:bool，默认为True;采样时是否替换替换，默认执行不替换的采样\n",
    " - bootstrap_features:bool，默认= False;是否替换特征\n",
    " - oob_score: bool，默认= False;是否使用现成的样本来估计泛化误差。\n",
    " - warm_start: bool，默认为False;设置为True时，请重用上一个调用的解决方案以适合并为集合添加更多估计量，否则，仅适合一个全新的集合。\n",
    " - n_jobs: int，默认=无;使用线程数。-1表示所有，可能会对可视化编程的监控线程造成影响\n",
    " - random_state: int或RandomState，默认=无;随机数种子\n",
    " - verbose: int, 默认为;训练和测试过程展示的详细程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.87202411])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=4,\n",
    "                       n_informative=2, n_targets=1,\n",
    "                       random_state=0, shuffle=False)\n",
    "regr = BaggingRegressor(base_estimator=SVR(),\n",
    "                        n_estimators=10, random_state=0).fit(X, y)\n",
    "regr.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林\n",
    "#### 随机森林分类器\n",
    "随机森林是一种元估计量，它适合数据集各个子样本上的许多决策树分类器，并使用平均数来提高预测准确性和控制过度拟合。子样本大小由max_samplesif参数 控制bootstrap=True（默认），否则整个数据集将用于构建每棵树。\n",
    "\n",
    "**参数：**\n",
    " - n_estimators： int，默认= 100;基础模型数量,默认100。\n",
    " \n",
    " - criterion：{“gini”, “entropy”}, 默认为”gini”;分割判断标准,基尼或者信息熵。\n",
    " \n",
    " - max_depth:int, 默认为None;树的最大深度,知道所有叶子都是纯净的,或者直到所有叶子都包含至少min_samples_split个样本。\n",
    " \n",
    " - min_samples_split:int or float, 默认为2;拆分内部节点所需的最少样本数,如果为整数则表示最小样本数,如果为float则表示百分比,ceil(min_samples_split * n_samples)。\n",
    " \n",
    " - min_samples_leaf:int or float, 默认为1;在叶节点处需要的最小样本数。仅在任何深度的分裂点在min_samples_leaf左分支和右分支中的每个分支上至少留下训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为整数则表示最小样本数,如果为float则表示百分比ceil(min_samples_leaf*min_samples_leaf)。\n",
    " \n",
    " - min_weight_fraction_leaf:float, 默认为0.0;在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。\n",
    " \n",
    " - max_features:int, float or {“auto”, “sqrt”, “log2”}, 默认为\"auto\";寻找最佳分割时要考虑的功能数量：如果为int，则max_features在每个拆分处考虑要素。如果为float，max_features则为小数，并 在每次拆分时考虑要素。int(max_features * n_features)。如果“自动”，则max_features=sqrt(n_features)。如果是“ sqrt”，则max_features=sqrt(n_features)。如果为“ log2”，则为max_features=log2(n_features)。如果没有，则max_features=n_features。注意：直到找到至少一个有效的节点样本分区，分割的搜索才会停止，即使它需要有效检查多个max_features要素也是如此。\n",
    " \n",
    " - max_leaf_nodes:int, 默认为None;以最佳的方式建立树。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。\n",
    " \n",
    " - min_impurity_decrease:float, 默认为0.0;如果节点分裂导致杂质减少大于或等于该值，则该节点将分裂。\n",
    " \n",
    " - min_impurity_split：float, 默认为None;树木生长尽早停止的阈值。如果节点的杂质高于阈值，则该节点将分裂，否则为叶。将于0.25删除。\n",
    " \n",
    " - bootstrap:bool，默认为True;采样时是否替换替换，默认执行不替换的采样。\n",
    " \n",
    " - oob_score: bool，默认= False;是否使用现成的样本来估计泛化误差。\n",
    " \n",
    " - warm_start: bool，默认为False;设置为True时，请重用上一个调用的解决方案以适合并为集合添加更多估计量，否则，仅适合一个全新的集合。\n",
    " \n",
    " - n_jobs: int，默认=无;使用线程数。-1表示所有，可能会对可视化编程的监控线程造成影响。\n",
    " \n",
    " - random_state: int或RandomState，默认=无;随机数种子。\n",
    " \n",
    " - verbose: int, 默认为;训练和测试过程展示的详细程度。\n",
    " \n",
    " - class_weight:dict, list of dict or “balanced”, 默认为None。与形式的类有关的权重。“平衡”模式使用y的值来自动调整与输入数据中的类频率成反比的权重，如下所示： n_samples / (n_classes * np.bincount(y))。对于多输出，y的每一列的权重将相乘。请注意，如果指定了sample_weight，则这些权重将与sample_weight（通过fit方法传递）相乘。\n",
    " \n",
    " - ccp_alpha:non-negative-float, 默认为0.0;用于最小化成本复杂性修剪的复杂性参数。具有最大成本复杂度的子树小于ccp_alpha所选择的子树 。默认情况下，不执行修剪。\n",
    " \n",
    " - max_samples:int或float，默认为None;如果bootstrap为True，则从X抽取以训练每个基本估计量的样本数。如果为无（默认），则抽取全部样本。如果为int，则抽取该数量(max_samples)的样本。如果为浮点数，则按比例抽取样品。max_samples * X.shape[0]max_samples(0, 1)。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=2, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExtraTreesClassifier\n",
    "参数与随机森林一致,不同之处在于决策树的阈值选择都是随机的,进一步提高了随机性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                     criterion='gini', max_depth=None, max_features='auto',\n",
       "                     max_leaf_nodes=None, max_samples=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_features=4, random_state=0)\n",
    "clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机森林回归\n",
    " \n",
    " **参数**\n",
    " - n_estimators int，默认= 100;基础模型数量,默认100。\n",
    " \n",
    " - criterion：{“mse”, “friedman_mse”, “mae”}, default=”mse”，分割时的衡量标准。\n",
    " \n",
    " - max_depth:int, 默认为None;树的最大深度,知道所有叶子都是纯净的,或者直到所有叶子都包含至少min_samples_split个样本。\n",
    " \n",
    " - min_samples_split:int or float, 默认为2;拆分内部节点所需的最少样本数,如果为整数则表示最小样本数,如果为float则表示百分比,ceil(min_samples_split * n_samples)。\n",
    " \n",
    " - min_samples_leaf:int or float, 默认为1;在叶节点处需要的最小样本数。仅在任何深度的分裂点在min_samples_leaf左分支和右分支中的每个分支上至少留下训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为整数则表示最小样本数,如果为float则表示百分比ceil(min_samples_leaf*min_samples_leaf)。\n",
    " \n",
    " - min_weight_fraction_leaf:float, 默认为0.0;在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。\n",
    " \n",
    " - max_features:int, float or {“auto”, “sqrt”, “log2”}, 默认为\"auto\";寻找最佳分割时要考虑的功能数量：如果为int，则max_features在每个拆分处考虑要素。如果为float，max_features则为小数，并 在每次拆分时考虑要素。int(max_features * n_features)。如果“自动”，则max_features=n_features。如果是“ sqrt”，则max_features=sqrt(n_features)。如果为“ log2”，则为max_features=log2(n_features)。如果没有，则max_features=n_features。注意：直到找到至少一个有效的节点样本分区，分割的搜索才会停止，即使它需要有效检查多个max_features要素也是如此。\n",
    " \n",
    " - max_leaf_nodes:int, 默认为None;以最佳的方式建立树。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。\n",
    " \n",
    " - min_impurity_decrease:float, 默认为0.0;如果节点分裂导致杂质减少大于或等于该值，则该节点将分裂。\n",
    " \n",
    " - min_impurity_split：float, 默认为None;树木生长尽早停止的阈值。如果节点的杂质高于阈值，则该节点将分裂，否则为叶。将于0.25删除\n",
    " \n",
    " - bootstrap:bool，默认为True;采样时是否替换替换，默认执行不替换的采样。\n",
    " \n",
    " - oob_score: bool，默认= False;是否使用现成的样本来估计泛化误差。\n",
    " \n",
    " - warm_start: bool，默认为False;设置为True时，请重用上一个调用的解决方案以适合并为集合添加更多估计量，否则，仅适合一个全新的集合。\n",
    " \n",
    " - n_jobs: int，默认=无;使用线程数。-1表示所有，可能会对可视化编程的监控线程造成影响。\n",
    " \n",
    " - random_state: int或RandomState，默认=无;随机数种子。\n",
    " \n",
    " - verbose: int, 默认为;训练和测试过程展示的详细程度。\n",
    " \n",
    " - max_samples:int或float，默认为None;如果bootstrap为True，则从X抽取以训练每个基本估计量的样本数。如果为无（默认），则抽取全部样本。如果为int，则抽取该数量(max_samples)的样本。如果为浮点数，则按比例抽取样品。max_samples * X.shape[0]max_samples(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_features=4, n_informative=2,\n",
    "                       random_state=0, shuffle=False)\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.32987858]\n"
     ]
    }
   ],
   "source": [
    "print(regr.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**极随机树**\n",
    "\n",
    "此类实现一个元估计器，该估计器可将多个随机决策树（又名额外树或极限树）拟合到数据集的各个子样本上，并使用平均数来提高预测准确性和控制过度拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27081747066124695"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n",
    "   X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost分类器。\n",
    "\n",
    "AdaBoost分类器是一种元估计器，它首先将分类器拟合到原始数据集上，然后将分类器的其他副本拟合到同一数据集上，但是其中，对错误分类的实例的权重进行了调整，以使后续分类器更加关注困难的情况。\n",
    "\n",
    "**参数**\n",
    " - base_estimator:对象，默认=无;基础模型（或称为estimator），默认为None，使用决策树。\n",
    " - n_estimators:int，默认= 10;基础模型数量。\n",
    " - learning_rate:float, default=1;学习率缩小了每个分类器的贡献,因此学习率和基础模型数量之间需要平衡。\n",
    " - algorithm:{‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’;如果为“ SAMME.R”，则使用SAMME.R实际增强算法。 base_estimator必须支持类概率的计算。如果为“ SAMME”，则使用SAMME离散提升算法。SAMME.R算法通常比SAMME收敛更快，从而以更少的提升迭代次数实现了更低的测试误差。\n",
    " - random_state: int或RandomState，默认=无;随机数种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost回归器\n",
    "\n",
    "AdaBoost回归器是一个元估计器，它首先将回归器拟合到原始数据集上，然后将回归器的其他副本拟合到同一数据集上，但根据当前预测的误差调整实例的权重。因此，随后的回归者更多地关注困难情况。此类实现称为AdaBoost.R2的算法。\n",
    "\n",
    "**参数:**\n",
    " - base_estimator:对象，默认=无;基础模型（或称为estimator），默认为None，使用决策树。\n",
    " - n_estimators:int，默认= 50;基础模型数量;\n",
    " - learning_rate:float, default=1;学习率缩小了每个分类器的贡献,因此学习率和基础模型数量之间需要平衡\n",
    " - loss: {'linear'，'square'，'exponential'}，默认=“ linear”;每次增强迭代后更新权重时使用的损失函数。\n",
    " - random_state: int或RandomState，默认=无;随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
       "                  n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_features=4, n_informative=2,\n",
    "                       random_state=0, shuffle=False)\n",
    "regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.79722349])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9771376939813696"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度提升\n",
    "梯度提升对任意微分损失函数进行增强的一种概括。GBDT是一种准确有效的现成程序，可用于Web搜索排名和生态学等多个领域的回归和分类问题。\n",
    "\n",
    "#### GradientBoostingClassifier\n",
    "\n",
    "GB以渐进的阶段方式建立加性模型；它允许优化任意微分损失函数。在每个阶段，n_classes_ 回归树都适合二项式或多项式偏差损失函数的负梯度。二进制分类是一种特殊情况，其中仅诱导单个回归树。\n",
    "\n",
    "**参数:**\n",
    " - loss;{‘deviance’, ‘exponential’}, 默认为’deviance’:损失函数有待优化。“偏差”是指对具有概率输出的分类的偏差（=逻辑回归）。对于损失，“指数”梯度提升可恢复AdaBoost算法。\n",
    " \n",
    " - learning_rate:float, 默认为0.1:学习率缩小了每棵树的贡献。在learning_rate和n_estimators之间需要权衡。\n",
    " \n",
    " - n_estimators:int, 默认为100:要执行的提升阶段数。梯度提升对于过度拟合具有相当强的鲁棒性，因此大量的提升阶数通常会带来更好的性能。\n",
    " \n",
    " - subsample:float, 默认为1.0:样本子采样,用于拟合各个基础学习器的样本比例。如果小于1.0，则将导致随机梯度增强。subsample与参数n_estimators交互。选择会导致方差减少和偏差增加。subsample < 1.0。\n",
    " \n",
    " - criterion:{‘friedman_mse’, ‘mse’, ‘mae’}, 默认为’friedman_mse’:衡量分割质量的功能。支持的标准是“ friedman_mse”（均方误差）和Friedman的改进评分，“ mse”（均方误差）和“ mae”（均绝对误差）。默认值'friedman_mse'通常是最好的，因为在某些情况下它可以提供更好的近似值。\n",
    " \n",
    " - min_samples_split:int or float, 默认为2:拆分内部节点所需的最少样本数：如果为int，则认为min_samples_split是最小值。如果为float，min_samples_split则为分数， 是每个拆分的最小样本数。ceil(min_samples_split * n_samples)。\n",
    " \n",
    " - min_samples_leaf:int or float, 默认为1:在叶节点处需要的最小样本数。仅在任何深度的分裂点在min_samples_leaf左分支和右分支中的每个分支上至少留下训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为int，则认为min_samples_leaf是最小值。如果为float，min_samples_leaf则为分数， 是每个节点的最小样本数。ceil(min_samples_leaf * n_samples)。\n",
    " \n",
    " - min_weight_fraction_leaf:float, 默认为0.0;在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。\n",
    " \n",
    " - max_depth:int, 默认为3:各个决策树的最大深度。最大深度限制了树中节点的数量。调整此参数以获得最佳性能；最佳值取决于各个参数的相互作用。\n",
    " - min_impurity_decrease:float, 默认为0.0:如果节点分裂导致杂质减少大于或等于该值，则该节点将分裂。\n",
    " \n",
    " - min_impurity_split:float, 默认为None:树木生长尽早停止的阈值。如果节点的杂质高于阈值，则该节点将分裂，否则为叶。将于0.25删除。\n",
    " \n",
    " - init:estimator or ‘zero’, 默认为None:一个估计器对象，用于计算初始预测。 init必须提供fit和predict_proba。如果为“零”，则初始原始预测设置为零。默认情况下，使用 DummyEstimator预测类优先级。\n",
    " \n",
    " - random_state:int or RandomState, 默认为None:随机数种子。\n",
    " \n",
    " - max_features:{‘auto’, ‘sqrt’, ‘log2’}, int or float, 默认为None:寻找最佳分割时要考虑的功能数量：如果为int，则max_features在每个拆分处考虑要素。如果为float，max_features则为小数，并 在每次拆分时考虑要素。int(max_features * n_features)。如果为“自动”，则为max_features=sqrt(n_features)。如果为“ sqrt”，则为max_features=sqrt(n_features)。如果为“ log2”，则为max_features=log2(n_features)。如果没有，则max_features=n_features。选择会导致方差减少和偏差增加。max_features < n_features。注意：直到找到至少一个有效的节点样本分区，分割的搜索才会停止，即使它需要有效检查多个max_features要素也是如此。\n",
    " \n",
    " - verbose:int, 默认为0:启用详细输出。如果为1，则偶尔打印一次进度和性能（树越多，频率越低）。如果大于1，则将打印每棵树的进度和性能。\n",
    " \n",
    " - max_leaf_nodes:bool, 默认为False;以最好的方式种植树木。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。\n",
    " \n",
    " - warm_start bool，默认为False;设置True为时，请重用上一个调用的解决方案以适应并在集合中添加更多的估计量，否则，只需擦除上一个解决方案即可。\n",
    " \n",
    " - presortde:precated, 默认为’deprecated’;此参数已弃用，并将在v0.24中删除。\n",
    " \n",
    " - validation_fraction:float, 默认为0.1;预留的训练数据比例作为早期停止的验证集。必须在0到1之间。仅当n_iter_no_change设置为整数时使用。\n",
    " \n",
    " - n_iter_no_change:int, 默认为None;n_iter_no_change用于确定在验证分数没有改善时是否将使用早期停止来终止训练。默认情况下，将其设置为“无”以禁用提前停止。如果设置为数字，则它将保留validation_fraction训练数据的大小作为验证，并在先前所有n_iter_no_change迭代次数中的验证得分均未提高时终止训练。拆分已分层。\n",
    " \n",
    " - tol:float, 默认为1e-4;早停阈值。当损失至少不能改善n_iter_no_change迭代次数（如果设置为数字）时，训练将停止。\n",
    " \n",
    " - ccp_alpha:non-negative float, 默认为0.0;用于最小化成本复杂性修剪的复杂性参数。具有最大成本复杂度的子树小于ccp_alpha所选择的子树 。默认情况下，不执行修剪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=0, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_classification(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingRegressor\n",
    "GB以渐进的阶段方式建立加性模型；它允许优化任意微分损失函数。在每个阶段，将回归树拟合到给定损失函数的负梯度上。\n",
    "\n",
    "**参数:**\n",
    " - loss;{'ls'，'lad'，'huber'，'quantile'}，默认='ls';损失函数有待优化。“ ls”是指最小二乘回归。“ lad”（最小绝对偏差）是仅基于输入变量的顺序信息的高度鲁棒的损失函数。“ huber”是两者的结合。“分位数”允许分位数回归（用于alpha指定分位数）。\n",
    " \n",
    " - learning_rate:float, 默认为0.1:学习率缩小了每棵树的贡献。在learning_rate和n_estimators之间需要权衡。\n",
    " \n",
    " - n_estimators:int, 默认为100:要执行的提升阶段数。梯度提升对于过度拟合具有相当强的鲁棒性，因此大量的提升阶数通常会带来更好的性能。\n",
    " \n",
    " - subsample:float, 默认为1.0:样本子采样,用于拟合各个基础学习器的样本比例。如果小于1.0，则将导致随机梯度增强。subsample与参数n_estimators交互。选择会导致方差减少和偏差增加。subsample < 1.0。\n",
    " \n",
    " - criterion:{‘friedman_mse’, ‘mse’, ‘mae’}, 默认为’friedman_mse’:衡量分割质量的功能。支持的标准是“ friedman_mse”（均方误差）和Friedman的改进评分，“ mse”（均方误差）和“ mae”（均绝对误差）。默认值'friedman_mse'通常是最好的，因为在某些情况下它可以提供更好的近似值。\n",
    " \n",
    " - min_samples_split:int or float, 默认为2:拆分内部节点所需的最少样本数：如果为int，则认为min_samples_split是最小值。如果为float，min_samples_split则为分数， 是每个拆分的最小样本数。ceil(min_samples_split * n_samples)。\n",
    " \n",
    " - min_samples_leaf:int or float, 默认为1:在叶节点处需要的最小样本数。仅在任何深度的分裂点在min_samples_leaf左分支和右分支中的每个分支上至少留下训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为int，则认为min_samples_leaf是最小值。如果为float，min_samples_leaf则为分数， 是每个节点的最小样本数。ceil(min_samples_leaf * n_samples)。\n",
    " \n",
    " - min_weight_fraction_leaf:float, 默认为0.0:在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。\n",
    " - max_depth:int, 默认为3:各个决策树的最大深度。最大深度限制了树中节点的数量。调整此参数以获得最佳性能；最佳值取决于各个参数的相互作用。\n",
    " - min_impurity_decrease:float, 默认为0.0:如果节点分裂导致杂质减少大于或等于该值，则该节点将分裂。\n",
    " \n",
    " - min_impurity_split:float, 默认为None:树木生长尽早停止的阈值。如果节点的杂质高于阈值，则该节点将分裂，否则为叶。将于0.25删除。\n",
    " \n",
    " - init:estimator or ‘zero’, 默认为None:一个估计器对象，用于计算初始预测。 init必须提供fit和predict_proba。如果为“零”，则初始原始预测设置为零。默认情况下，使用 DummyEstimator预测类优先级。\n",
    " \n",
    " - random_state:int or RandomState, 默认为None:随机数种子。\n",
    " \n",
    " - max_features:{‘auto’, ‘sqrt’, ‘log2’}, int or float, 默认为None:寻找最佳分割时要考虑的功能数量：如果为int，则max_features在每个拆分处考虑要素。如果为float，max_features则为小数，并 在每次拆分时考虑要素。int(max_features * n_features)。如果为“自动”，则为max_features=sqrt(n_features)。如果为“ sqrt”，则为max_features=sqrt(n_features)。如果为“ log2”，则为max_features=log2(n_features)。如果没有，则max_features=n_features。选择会导致方差减少和偏差增加。max_features < n_features。注意：直到找到至少一个有效的节点样本分区，分割的搜索才会停止，即使它需要有效检查多个max_features要素也是如此。\n",
    " \n",
    " - alphafloat, default=0.9;Huber损失函数和分位数损失函数的alpha分位数。仅当loss='huber'或时loss='quantile'。\n",
    " \n",
    " - verbose:int, 默认为0:启用详细输出。如果为1，则偶尔打印一次进度和性能（树越多，频率越低）。如果大于1，则将打印每棵树的进度和性能。\n",
    " \n",
    " - max_leaf_nodes:bool, 默认为False;以最好的方式种植树木。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。\n",
    " \n",
    " - warm_start bool，默认为False;设置True为时，请重用上一个调用的解决方案以适应并在集合中添加更多的估计量，否则，只需擦除上一个解决方案即可。\n",
    " - presortde:precated, 默认为’deprecated’;此参数已弃用，并将在v0.24中删除。\n",
    " \n",
    " - validation_fraction:float, 默认为0.1;预留的训练数据比例作为早期停止的验证集。必须在0到1之间。仅当n_iter_no_change设置为整数时使用。\n",
    " \n",
    " - n_iter_no_change:int, 默认为None;n_iter_no_change用于确定在验证分数没有改善时是否将使用早期停止来终止训练。默认情况下，将其设置为“无”以禁用提前停止。如果设置为数字，则它将保留validation_fraction训练数据的大小作为验证，并在先前所有n_iter_no_change迭代次数中的验证得分均未提高时终止训练。拆分已分层。\n",
    " \n",
    " - tol:float, 默认为1e-4;早停阈值。当损失至少不能改善n_iter_no_change迭代次数（如果设置为数字）时，训练将停止。\n",
    " \n",
    " - ccp_alpha:non-negative float, 默认为0.0;用于最小化成本复杂性修剪的复杂性参数。具有最大成本复杂度的子树小于ccp_alpha所选择的子树 。默认情况下，不执行修剪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=0, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-61.05212593])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(X_test[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43848663277068134"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
