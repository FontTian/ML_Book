{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成学习\n",
    "本部分主要讲的是非skleran的集成算法,目前主要包括XGB与LGT,由于这两个包都是多语言多风格API,而Index主要是为了longriver中的PythonML部分,因此这里仅讲XGB与LGT的skleranAPI部分.更多内容请直接参考文档,github或本人的其他notebooks.\n",
    "\n",
    "### XGB\n",
    "XGBoost是一个优化的分布式梯度增强库，旨在实现高效，灵活和便携。它在 Gradient Boosting 框架下实现机器学习算法。XGBoost提供并行树提升（也称为GBDT，GBM），可以快速准确地解决许多数据科学问题。相同的代码在主要的分布式环境（Hadoop，SGE，MPI）上运行，并且可以解决数十亿个示例之外的问题。\n",
    "### XGBClassifier\n",
    "**参数:**\n",
    " - max_depth:int,默认为None;基础学习器的最大树深度。\n",
    " - learning_rate:float,默认为None;学习率。\n",
    " - verbosity:int,详细程度,0表示静默,3(调试)。\n",
    " - objective:字符串,或可调用的类,默认为\"binary:logistic\";指定学习任务和要使用的相应学习目标或自定义目标函数。\n",
    " - booster:字符串,制定要使用的助推器,{gbtree，gblinear,dart}之一。\n",
    " - tree_method:(字符串)指定要使用的树方法。默认为自动。如果将此参数设置为默认值，则XGBoost将选择最保守的选项。建议从参数文档中研究此选项。\n",
    " - n_jobs:用于运行xgboost的并行线程数,注意使用全部线程数可能会对可视化编辑器的监控线程造成影响。\n",
    " - gamma:(float)在树的叶节点上进行进一步分支所需的最小损失减少。\n",
    " - min_child_weight:(int)子树中所需实例权重（hessian）的最小总和。\n",
    " - max_delta_step:(int)我们允许每棵树的权重估计为最大增量步长。\n",
    " - subsample:(float)子样本(样本子采样)的比例。\n",
    " - colsample_bytree:(float)构造每棵树时列的子采样率。\n",
    " - colsample_bylevel:(float)每个子级(子叶，level)的列的子采样率。\n",
    " - colsample_bynode:(float),每次分裂(split)的子采样率。\n",
    " - reg_alpha:(float)L1正则化权重。\n",
    " - reg_lambda:(float)L2正则化权重。\n",
    " - scale_pos_weight:(float)平衡正负权重。\n",
    " - base_score:所有实例的初始预测分数，全局偏差。\n",
    " - random_state:随机数种子。\n",
    " - missing:(float,默认为np.nan)如何对数据中的缺失值进行处理。\n",
    " - num_parallel_tree:(int)用于增强随机森林。\n",
    " - monotone_constraints:(str)可变单调性的约束。有关更多信息，请参见XGB官方教程。\n",
    " - interaction_constraints:交互约束，表示允许的交互。约束必须以嵌套列表的形式指定，例如[[0，1]，[2、3、4]]，其中每个内部列表都是一组允许相互交互的要素索引。有关更多信息，请参见XGB官方教程。\n",
    " - importance_type:（字符串，默认为“ gain”）– feature_importances_属性的特征重要性类型：“ gain”，“ weight”，“ cover”，“ total_gain”或“ total_cover”。\n",
    " \n",
    "另外比较重要的参数还有fit时的参数\n",
    " - sample_weight:实例权重。\n",
    " - base_margin:每个实例的全局偏差。\n",
    " - eval_set:用作验证集的（X，y）元组对的列表，将为其计算指标。验证指标将帮助我们跟踪模型的性能。\n",
    " - eval_metric:评价函数,str或者str列表,又或者自定义的可调用方法 。\n",
    " - early_stopping_rounds:int,早停回合数;如果eval_set中有多个项目，则最后一个条目将用于提前停止。如果eval_metric中有多个指标，则最后一个指标将用于提前停止。如果发生提早终止，该模型将有三个附加字段： clf.best_score，clf.best_iteration和clf.best_ntree_limit。\n",
    " - 等等。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.01111\tvalidation_1-merror:0.11667\n",
      "[1]\tvalidation_0-merror:0.01111\tvalidation_1-merror:0.05000\n"
     ]
    }
   ],
   "source": [
    "param_dist = {'objective':'multi:softmax', 'n_estimators':2}\n",
    "\n",
    "clf = xgb.XGBClassifier(**param_dist)\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        eval_metric='merror',\n",
    "        verbose=True)\n",
    "\n",
    "evals_result = clf.evals_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBRegressor\n",
    "\n",
    "**参数:**\n",
    " - n_estimators:int,默认为100;梯度增强树的数量,默认为100。\n",
    " - max_depth:int,基础学习器的最大树深度,默认为None。\n",
    " - learning_rate:float,学习率,默认为None。\n",
    " - verbosity:int,详细程度,0表示静默,3(调试)。\n",
    " - objective:字符串,或可调用的类,默认为reg:squarederror;指定学习任务和要使用的相应学习目标或自定义目标函数,默认为None。\n",
    " - booster:字符串,制定要使用的助推器,{gbtree，gblinear,dart}之一,默认为None。\n",
    " - tree_method:(字符串)指定要使用的树方法。默认为自动。如果将此参数设置为默认值，则XGBoost将选择最保守的选项。建议从参数文档中研究此选项,默认为None。\n",
    " - n_jobs:用于运行xgboost的并行线程数,注意使用全部线程数可能会对可视化编辑器的监控线程造成影响,默认为None。\n",
    " - gamma:(float)在树的叶节点上进行进一步分支所需的最小损失减少,默认为None。\n",
    " - min_child_weight:(int)子树中所需实例权重（hessian的最小总和）,默认为None。\n",
    " - max_delta_step:(int)我们允许每棵树的权重估计为最大增量步长,默认为None。\n",
    " - subsample:(float)子样本(样本子采样)的比例,默认为None。\n",
    " - colsample_bytree:(float)构造每棵树时列的子采样率,默认为None。\n",
    " - colsample_bylevel:(float)每个子级（子叶，level)的列的子采样率,默认为None。\n",
    " - colsample_bynode:(float),每次分裂(split)的子采样率,默认为None。\n",
    " - reg_alpha:(float)L1正则化权重,默认为None。\n",
    " - reg_lambda:(float)L2正则化权重,默认为None。\n",
    " - scale_pos_weight:(float)平衡正负权重,默认为None。\n",
    " - base_score:所有实例的初始预测分数，全局偏差,默认为None。\n",
    " - random_state:随机数种子,默认为None。\n",
    " - missing:(float,默认为np.nan)如何对数据中的缺失值进行处理。\n",
    " - num_parallel_tree:(int)用于增强随机森林,默认为None。\n",
    " - monotone_constraints:(str)可变单调性的约束。有关更多信息，请参见XGB官方教程,默认为None。\n",
    " - interaction_constraints:交互约束，表示允许的交互。约束必须以嵌套列表的形式指定，例如[[0，1]，[2、3、4]]，其中每个内部列表都是一组允许相互交互的要素索引。有关更多信息，请参见XGB官方教程,默认为None。\n",
    " - importance_type:（字符串，默认为“ gain”）– feature_importances_属性的特征重要性类型：“ gain”，“ weight”，“ cover”，“ total_gain”或“ total_cover”。\n",
    " \n",
    "另外比较重要的参数还有fit时的参数\n",
    " - sample_weight:实例权重。\n",
    " - base_margin:每个实例的全局偏差。\n",
    " - eval_set:用作验证集的（X，y）元组对的列表，将为其计算指标。验证指标将帮助我们跟踪模型的性能。\n",
    " - eval_metric:评价函数,str或者str列表,又或者自定义的可调用方法 。\n",
    " - early_stopping_rounds:int,早停回合数;如果eval_set中有多个项目，则最后一个条目将用于提前停止。如果eval_metric中有多个指标，则最后一个指标将用于提前停止。如果发生提早终止，该模型将有三个附加字段： clf.best_score，clf.best_iteration和clf.best_ntree_limit。\n",
    " - 等等。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_features=6, n_informative=200,\n",
    "                       random_state=0, shuffle=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) \n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_0': {'rmse': [122.213531, 94.250137]},\n",
       " 'validation_1': {'rmse': [153.01265, 147.528229]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "param_dist = {'objective':'reg:squarederror', 'n_estimators':2}\n",
    "\n",
    "clf = xgb.XGBRegressor(**param_dist)\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        eval_metric='rmse',\n",
    "        verbose=False)\n",
    "\n",
    "evals_result = clf.evals_result()\n",
    "evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -51.918674 ,   -8.8689375,    9.159288 ,   77.61025  ,\n",
       "          9.868956 ,    5.534939 ,  -36.90695  ,   10.067278 ,\n",
       "        -71.28563  ,   -7.065931 ,  -52.85189  ,  -51.918674 ,\n",
       "        -85.085915 ,  -52.85189  ,  -71.28563  ,  -52.85189  ,\n",
       "        -52.85189  ,  159.87268  ,  159.87268  , -117.475296 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM\n",
    "\n",
    "LightGBM是使用基于树的学习算法的梯度增强框架。它被设计为分布式且高效的。\n",
    "\n",
    "#### LGBMClassifier\n",
    "**参数:**\n",
    " - boosting_type:字符串,默认'gbdt';–'gbdt'，传统的梯度增强决策树。'dart'，辍学遇到多个加性回归树。基于梯度的“ goss”单边采样。“ rf”，随机森林。\n",
    " \n",
    " - num_leaves:int,默认为31;基础学习器的最大叶数量。\n",
    " \n",
    " - max_depth:int,default=-1;基础学习器的最大树深度,<0表示无限制。\n",
    " \n",
    " - learning_rate:float,默认为0.1;学习率。\n",
    " \n",
    " - n_estimators:int,默认为100;梯度增强树的数量。\n",
    " \n",
    " - subsample_for_bin:int,默认为200000;用于构造箱子(bin)的数量。\n",
    " \n",
    " - objective:字符串,或可调用的类默认为None;指定学习任务和要使用的相应学习目标或自定义目标函数;默认值：LGBMRegressor为'regression'，LGBMClassifier为'binary'或'multiclass'，LGBMRanker为'lambdarank'。\n",
    " \n",
    " - n_jobs:用于运行xgboost的并行线程数，默认-1;,注意使用全部线程数可能会对可视化编辑器的监控线程造成影响。\n",
    " \n",
    " - min_split_gain:float,默认为0;在叶节点上进一步进行分支所需要的最小损失减少。\n",
    " \n",
    " - min_child_weight:(float,默认为1e-3))子级(叶)中所需实例权重（hessian）的最小总和。\n",
    " \n",
    " - min_child_samples:int,默认20,子级(叶)中所需的最小数据数。\n",
    " \n",
    " - subsample：float，默认1;训练样本子样本的比率。\n",
    " \n",
    " - subsample_freq：int，默认0;子样本的频率;<=0表示没有启用。\n",
    " \n",
    " - colsample_bytree:(float,默认=1)构造每棵树时列的子采样率。\n",
    " - reg_alpha:(float,默认为0)L1正则化权重。\n",
    " - reg_lambda:(float,默认为0)L2正则化权重。\n",
    " \n",
    " - random_state:随机数种子。\n",
    " - silent:bool默认True,开启增强运行时的打印信息。\n",
    " - importance_type:（字符串，默认为“ gain”）– feature_importances_属性的特征重要性类型：如果为“ split”，则结果包含该特征在模型中使用的次数。如果为“ gain”，则结果包含使用该功能的分割的总增益。\n",
    " \n",
    "训练时比较重要的参数：\n",
    " - early_stopping_rounds：（int 或None ，可选（default = None ））–激活提前停止。该模型将进行训练，直到验证分数停止提高。验证分数至少需要提高early_stopping_rounds每轮才能继续训练。需要至少一个验证数据和一个指标。如果超过一个，将检查所有这些。但是无论如何，训练数据都被忽略了。要仅检查第一个度量，请将first_metric_only参数设置为模型构造函数的True。\n",
    " - 等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGBMRegressor\n",
    "\n",
    "**参数:**\n",
    " - boosting_type:字符串,默认'gbdt';–'gbdt'，传统的梯度增强决策树。'dart'，辍学遇到多个加性回归树。基于梯度的“ goss”单边采样。“ rf”，随机森林。\n",
    " \n",
    " - num_leaves:int,默认为31;基础学习器的最大叶数量。\n",
    " \n",
    " - max_depth:int,default=-1;基础学习器的最大树深度,<0表示无限制。\n",
    " \n",
    " - learning_rate:float,默认为0.1;学习率。\n",
    " \n",
    " - n_estimators:int,默认为100;梯度增强树的数量。\n",
    " \n",
    " - subsample_for_bin:int,默认为200000;用于构造箱子(bin)的数量。\n",
    " \n",
    " - objective:字符串,或可调用的类默认为None;指定学习任务和要使用的相应学习目标或自定义目标函数;默认值：LGBMRegressor为'regression'，LGBMClassifier为'binary'或'multiclass'，LGBMRanker为'lambdarank'。\n",
    " \n",
    " - n_jobs:用于运行xgboost的并行线程数，默认-1;,注意使用全部线程数可能会对可视化编辑器的监控线程造成影响。\n",
    " \n",
    " - min_split_gain:float,默认为0;在叶节点上进一步进行分支所需要的最小损失减少。\n",
    " \n",
    " - min_child_weight:(float,默认为1e-3))子级(叶)中所需实例权重（hessian）的最小总和。\n",
    " \n",
    " - min_child_samples:int,默认20,子级(叶)中所需的最小数据数。\n",
    " \n",
    " - subsample_freq：int，默认0;子样本的频率;<=0表示没有启用。\n",
    " \n",
    " - colsample_bytree:(float,默认=1)构造每棵树时列的子采样率。\n",
    " - reg_alpha:(float,默认为0)L1正则化权重。\n",
    " - reg_lambda:(float,默认为0)L2正则化权重。\n",
    " \n",
    " - random_state:随机数种子。\n",
    " - silent:bool默认True,开启增强运行时的打印信息。\n",
    " - importance_type:（字符串，默认为“ gain”）– feature_importances_属性的特征重要性类型：如果为“ split”，则结果包含该特征在模型中使用的次数。如果为“ gain”，则结果包含使用该功能的分割的总增益。\n",
    " \n",
    "训练时比较重要的参数：\n",
    " - early_stopping_rounds：（int 或None ，可选（default = None ））–激活提前停止。该模型将进行训练，直到验证分数停止提高。验证分数至少需要提高early_stopping_rounds每轮才能继续训练。需要至少一个验证数据和一个指标。如果超过一个，将检查所有这些。但是无论如何，训练数据都被忽略了。要仅检查第一个度量，请将first_metric_only参数设置为模型构造函数的True。\n",
    " - 等等"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
