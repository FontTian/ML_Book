{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "\n",
    "人工神经网络：是一种应用类似于大脑神经突触联接的结构进行信息处理的数学模型。在工程与学术界也常直接简称为“神经网络”或类神经网络。\n",
    "\n",
    "MLP在两个数组上训练：大小为（n_samples，n_features）的数组X，它保存表示为浮点特征向量的训练样本；和数组y的大小（n_samples，），其中包含训练样本的目标值（类标签）\n",
    "\n",
    "### 多层感知器分类\n",
    "\n",
    "多层感知器分类器,该模型使用LBFGS或随机梯度下降来优化对数损失函数。\n",
    "\n",
    "**参数:**\n",
    "\n",
    " - hidden_layer_sizes:元组，长度= n_layers-2，默认值=（100，）。\n",
    " \n",
    " - activation:{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认为’relu’;隐藏层的激活函数.‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x;‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x));‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x);‘relu’, the rectified linear unit function, returns f(x) = max(0, x)。\n",
    " \n",
    " - solver:{‘lbfgs’, ‘sgd’, ‘adam’}, 默认为’adam’,优化器;“ lbfgs”是准牛顿方法族的优化程序。“ sgd”是指随机梯度下降。“ adam”是指Kingma，Diederik和Jimmy Ba提出的基于随机梯度的优化器;注意：就训练时间和验证分数而言，默认求解器“ adam”在相对较大的数据集（具有数千个训练样本或更多）上的效果很好。但是，对于小型数据集，“ lbfgs”可以收敛得更快并且性能更好。\n",
    " \n",
    " - alpha:float,默认为0.0001;L2惩罚项的参数。\n",
    " \n",
    " - batch_size:int,默认为\"auto\"。随机优化器的迷你批次的大小。如果求解器为“ lbfgs”，则分类器将不使用迷你批处理。设为“自动（auto）”时，batch_size=min(200, n_samples)。\n",
    " \n",
    " - learning_rate：{'constant'，'invscaling'，'adaptive'}，默认为'constant'，学习率。“常数（constant）”是由“ learning_rate_init”给出的恒定学习率。“反比例缩放（invscaling）”使用反比例缩放指数“ power_t”在每个时间步“ t”处逐渐降低学习率。Effective_learning_rate = learning_rate_init / pow（t，power_t）。只要训练损失不断减少，“自适应（adaptive）”就可以将学习率保持恒定为“ learning_rate_init”。如果“ early_stopping”开启，则每次连续两个纪元未能将训练损失减少至少tol，或未能将验证分数增加至少破坏，则将当前学习率除以5。\n",
    " \n",
    " - learning_rate_init：int，默认值= 0.001。使用的初始学习率。它在更新权重时控制步长。仅在Solver ='sgd'或'adam'时使用。\n",
    " \n",
    " - power_t：int，默认= 0.5。反比例学习率的指数。当learning_rate设置为“ invscaling”时，它用于更新有效学习率。仅在Solver ='sgd'时使用。\n",
    " \n",
    " - max_iter：int，默认= 200；最大迭代次数。求解器迭代直到收敛（由“ tol”确定）或此迭代次数。对于随机解算器（“ sgd”，“ adam”），请注意，这决定了历元数（每个数据点将使用多少次），而不是梯度步数。\n",
    " \n",
    " - shuffle：bool，默认= True；是否在每次迭代中随机播放样本。仅在Solver ='sgd'或'adam'时使用。\n",
    " \n",
    " - random_state：int，RandomState实例，默认=None；随机数种子。\n",
    " \n",
    " - tol：float，默认= 1e-4；优化公差。当损失或得分至少tol在n_iter_no_change连续迭代中没有改善时，除非learning_rate将其设置为“自适应”，否则视为收敛，并且训练停止。\n",
    " \n",
    " - warm_start：bool，默认为False；设置为True时，请重用上一个调用的解决方案以适合初始化，否则，只需擦除以前的解决方案即可。\n",
    " \n",
    " - momentum：浮点型，默认值= 0.9；梯度下降更新的动量。应该在0到1之间。仅在solver ='sgd'时使用。\n",
    " \n",
    " - nesterovs_momentum：布尔值，默认为True；是否使用nesterovs’momentum。仅在Solver ='sgd'且动量> 0时使用。\n",
    " \n",
    " - early_stopping:bool，默认为False；当验证分数没有提高时是否使用提前停止来终止训练。如果设置为true，它将自动预留10％的训练数据作为验证，并在n_iter_no_change连续几个时期验证分数没有改善至少tol时终止训练 。除多标签设置外，拆分是分层的。仅在Solver ='sgd'或'adam'时有效。\n",
    " \n",
    " - validation_fraction:浮点型，默认为0.1；预留的训练数据比例作为早期停止的验证集。必须在0到1之间。仅当early_stopping为True时使用。\n",
    " \n",
    " - beta_1:float，默认= 0.9；用于估计亚当中第一矩矢量的指数衰减率应为`[0，1）`。仅在solver ='adam'时使用。\n",
    " \n",
    " - beta_2:float，默认= 0.999；用于估计亚当中第二矩矢量的指数衰减率应为`[0，1）`。仅在solver ='adam'时使用。\n",
    " \n",
    " - epsilon:float，默认值= 1e-8；亚当数值稳定性的值。仅在solver ='adam'时使用。\n",
    " \n",
    " - n_iter_no_change：int，默认值为10；无法满足tol改进的最大时期数。仅在Solver ='sgd'或'adam'时有效。\n",
    " \n",
    " - max_fun：int，默认= 15000；仅在Solver ='lbfgs'时使用。丢失函数调用的最大次数。求解器迭代直到收敛（由“ tol”确定），迭代次数达到max_iter或损失函数调用的次数为止。请注意，损失函数调用的次数将大于或等于 `MLPClassifier` 的迭代次数MLPClassifier。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01397002, 0.98602998]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_classification(n_samples=100, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=1)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100),random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "\n",
    "MLP在两个数组上训练：大小为（n_samples，n_features）的数组X，它保存表示为浮点特征向量的训练样本；和数组y的大小（n_samples，），其中包含训练样本的目标值（类标签）\n",
    "\n",
    "### 多层感知器分类\n",
    "\n",
    "多层感知器分类器,该模型使用LBFGS或随机梯度下降来优化对数损失函数。\n",
    "\n",
    "**参数:**\n",
    "\n",
    " - hidden_layer_sizes:元组，长度= n_layers-2，默认值=（100，）\n",
    " \n",
    " - activation:{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认为’relu’;隐藏层的激活函数.‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x;‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x));‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x);‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n",
    " \n",
    " - solver{‘lbfgs’, ‘sgd’, ‘adam’}, 默认为’adam’,优化器;“ lbfgs”是准牛顿方法族的优化程序。“ sgd”是指随机梯度下降。“ adam”是指Kingma，Diederik和Jimmy Ba提出的基于随机梯度的优化器;注意：就训练时间和验证分数而言，默认求解器“ adam”在相对较大的数据集（具有数千个训练样本或更多）上的效果很好。但是，对于小型数据集，“ lbfgs”可以收敛得更快并且性能更好。\n",
    " \n",
    " - alpha:float,默认为0.0001;L2惩罚项的参数。\n",
    " \n",
    " - batch_size:int,默认为\"auto\"。随机优化器的迷你批次的大小。如果求解器为“ lbfgs”，则分类器将不使用迷你批处理。设为“自动（auto）”时，batch_size=min(200, n_samples)。\n",
    " \n",
    " - learning_rate：{'constant'，'invscaling'，'adaptive'}，默认为'constant'，学习率。“常数（constant）”是由“ learning_rate_init”给出的恒定学习率。“反比例缩放（invscaling）”使用反比例缩放指数“ power_t”在每个时间步“ t”处逐渐降低学习率。Effective_learning_rate = learning_rate_init / pow（t，power_t）。只要训练损失不断减少，“自适应（adaptive）”就可以将学习率保持恒定为“ learning_rate_init”。如果“ early_stopping”开启，则每次连续两个纪元未能将训练损失减少至少tol，或未能将验证分数增加至少破坏，则将当前学习率除以5。\n",
    " \n",
    " - learning_rate_init：int，默认值= 0.001。使用的初始学习率。它在更新权重时控制步长。仅在Solver ='sgd'或'adam'时使用。\n",
    " \n",
    " - power_t double，默认= 0.5。反比例学习率的指数。当learning_rate设置为“ invscaling”时，它用于更新有效学习率。仅在Solver ='sgd'时使用。\n",
    " \n",
    " - max_iter int，默认= 200；最大迭代次数。求解器迭代直到收敛（由“ tol”确定）或此迭代次数。对于随机解算器（“ sgd”，“ adam”），请注意，这决定了历元数（每个数据点将使用多少次），而不是梯度步数。\n",
    " \n",
    " - shuffle：bool，默认= True；是否在每次迭代中随机播放样本。仅在Solver ='sgd'或'adam'时使用。\n",
    " \n",
    " - random_state：int，RandomState实例，默认=None；随机数种子。\n",
    " \n",
    " - tol：float，默认= 1e-4；优化公差。当损失或得分至少tol在n_iter_no_change连续迭代中没有改善时，除非learning_rate将其设置为“自适应”，否则视为收敛，并且训练停止。\n",
    " \n",
    " - warm_start bool，默认为False；设置为True时，请重用上一个调用的解决方案以适合初始化，否则，只需擦除以前的解决方案即可。\n",
    " \n",
    " - momentum：浮点型，默认值= 0.9；梯度下降更新的动量。应该在0到1之间。仅在solver ='sgd'时使用。\n",
    " \n",
    " - nesterovs_momentum：布尔值，默认为True；是否使用nesterovs’momentum。仅在Solver ='sgd'且动量> 0时使用。\n",
    " \n",
    " - early_stopping:bool，默认为False；当验证分数没有提高时是否使用提前停止来终止训练。如果设置为true，它将自动预留10％的训练数据作为验证，并在n_iter_no_change连续几个时期验证分数没有改善至少tol时终止训练 。除多标签设置外，拆分是分层的。仅在Solver ='sgd'或'adam'时有效。\n",
    " \n",
    " - validation_fraction:浮点型，默认为0.1；预留的训练数据比例作为早期停止的验证集。必须在0到1之间。仅当early_stopping为True时使用。\n",
    " \n",
    " - beta_1:float，默认= 0.9；用于估计亚当中第一矩矢量的指数衰减率应为`[0，1）`。仅在solver ='adam'时使用。\n",
    " \n",
    " - beta_2:float，默认= 0.999；用于估计亚当中第二矩矢量的指数衰减率应为`[0，1）`。仅在solver ='adam'时使用。\n",
    " \n",
    " - epsilon:float，默认值= 1e-8；亚当数值稳定性的值。仅在solver ='adam'时使用。\n",
    " \n",
    " - n_iter_no_change：int，默认值为10；无法满足tol改进的最大时期数。仅在Solver ='sgd'或'adam'时有效。\n",
    " \n",
    " - max_fun：int，默认= 15000；仅在Solver ='lbfgs'时使用。丢失函数调用的最大次数。求解器迭代直到收敛（由“ tol”确定），迭代次数达到max_iter或损失函数调用的次数为止。请注意，损失函数调用的次数将大于或等于 `MLPRegressor` 的迭代次数MLPClassifier。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fonttian/anaconda3/envs/keras/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.98506347, -7.19854141])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(n_samples=200, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=1)\n",
    "regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "regr.predict(X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4162338898076593"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
